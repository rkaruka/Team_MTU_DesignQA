{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMb89q5bXse"
      },
      "outputs": [],
      "source": [
        "!pip -q install pillow pymupdf sentence-transformers faiss-cpu rank_bm25 easyocr rapidfuzz pint openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, sys, textwrap, traceback, importlib.util\n",
        "import fitz, faiss, cv2, easyocr\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from rapidfuzz import process\n",
        "from pint import UnitRegistry\n",
        "import nltk; nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "id": "yWurNnz8fx8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "#My Google collab\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY environment variable. Please set it before running.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "O1m-aWlAikNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"/content/rule_functional_performance_qa.csv\"\n",
        "IMG_DIR = Path(\"/content/images\")\n",
        "\n",
        "if not IMG_DIR.exists():\n",
        "    os.system(\"git clone -q https://github.com/anniedoris/design_qa.git /content/design_qa\")\n",
        "    os.makedirs(IMG_DIR, exist_ok=True)\n",
        "    os.system(\"cp -r /content/design_qa/dataset/rule_compliance/rule_functional_performance_qa/images/* /content/images/\")\n",
        "\n",
        "print(\"Images ready:\", len(list(IMG_DIR.glob('*.png'))))\n",
        "\n",
        "if not Path(CSV_PATH).exists():\n",
        "    print(\"Downloading Functional Performance CSV from DesignQA GitHub...\")\n",
        "    os.system(\"wget -q -O /content/rule_functional_performance_qa.csv \"\n",
        "              \"https://raw.githubusercontent.com/anniedoris/design_qa/main/dataset/rule_compliance/rule_functional_performance_qa/rule_functional_performance_qa.csv\")\n",
        "\n",
        "assert Path(CSV_PATH).exists(), f\"Functional Performance CSV still missing at {CSV_PATH}\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(f\"CSV loaded: {len(df)} rows\")\n",
        "RULE_PDF = \"/content/FSAE_Rules_2024_V1.pdf\"\n",
        "if not Path(RULE_PDF).exists():\n",
        "    from google.colab import files\n",
        "    print(\"Upload your FSAE_Rules_2024_V1.pdf\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        up_name = list(uploaded.keys())[0]\n",
        "        os.replace(up_name, RULE_PDF)\n",
        "assert Path(RULE_PDF).exists(), \"FSAE PDF missing.\""
      ],
      "metadata": {
        "id": "WMcOrMITcoAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_text(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    txt = []\n",
        "    for p in doc:\n",
        "        t = p.get_text(\"text\")\n",
        "        txt.append(t)\n",
        "    full = \"\\n\".join(txt)\n",
        "    full = full.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    full = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', full)\n",
        "    full = re.sub(r'–|—', '-', full)\n",
        "    full = re.sub(r'[ \\t]+', ' ', full)\n",
        "    full = re.sub(r'\\n{3,}', '\\n\\n', full)\n",
        "    return full\n",
        "\n",
        "HEADER_RES = [\n",
        "    re.compile(r'^\\s*Formula SAE.*Page\\s+\\d+\\s+of\\s+\\d+\\s*$', re.I),\n",
        "    re.compile(r'^\\s*Version\\s+\\d+(\\.\\d+)?\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s*$', re.I),\n",
        "]\n",
        "TOC_LINE_RE       = re.compile(r'.+\\.\\s?\\.\\s?\\.\\s+\\d+$')\n",
        "SECTION_BANNER_RE = re.compile(r'^[A-Z]{1,4}\\s*-\\s+.+$')\n",
        "RULE_HEAD_ANCHOR  = re.compile(r'(?m)^(?:EV|T|F|GR)\\.\\d+(?:\\.\\d+)*[a-z]?\\b')\n",
        "\n",
        "def strip_noise(raw_text):\n",
        "    out = []\n",
        "    for ln in raw_text.splitlines():\n",
        "        s = ln.rstrip().replace('\\xa0', ' ')\n",
        "        if any(rx.match(s) for rx in HEADER_RES): continue\n",
        "        if TOC_LINE_RE.search(s): continue\n",
        "        if SECTION_BANNER_RE.match(s): continue\n",
        "        out.append(s)\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "def drop_until_first_rule(text):\n",
        "    m = RULE_HEAD_ANCHOR.search(text)\n",
        "    return text[m.start():] if m else text\n",
        "\n",
        "RULE_HEAD_RE = re.compile(\n",
        "    r'(?m)^(?P<rid>[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*[a-z]?)(?:[ \\t]+(?P<title>.+))?$'\n",
        ")\n",
        "\n",
        "def parse_rules_from_text(text):\n",
        "    lines = text.splitlines()\n",
        "    rules = []\n",
        "    cur_id, buf = None, []\n",
        "\n",
        "    def flush():\n",
        "        nonlocal cur_id, buf, rules\n",
        "        if cur_id and buf:\n",
        "            content = \"\\n\".join(buf).strip()\n",
        "            if content:\n",
        "                if not content.startswith(cur_id):\n",
        "                    content = f\"{cur_id} \" + content\n",
        "                rules.append((cur_id, re.sub(r'\\s+', ' ', content)))\n",
        "        cur_id, buf = None, []\n",
        "\n",
        "    for ln in lines:\n",
        "        s = ln.strip()\n",
        "        m = RULE_HEAD_RE.match(s)\n",
        "        if m:\n",
        "            flush()\n",
        "            cur_id = m.group(\"rid\").strip()\n",
        "            buf = [s]\n",
        "        else:\n",
        "            if cur_id:\n",
        "                buf.append(ln)\n",
        "    flush()\n",
        "    return rules\n",
        "\n",
        "raw_text   = read_pdf_text(RULE_PDF)\n",
        "clean_text = strip_noise(raw_text)\n",
        "clean_text = drop_until_first_rule(clean_text)\n",
        "rule_pairs = parse_rules_from_text(clean_text)\n",
        "rule_chunks = {rid: txt for rid, txt in rule_pairs}\n",
        "print(\"Total rules parsed:\", len(rule_chunks))"
      ],
      "metadata": {
        "id": "ksDz8wRHhBKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ureg = UnitRegistry()\n",
        "Q_ = ureg.Quantity\n",
        "def _clean_unit(u: str) -> str:\n",
        "    return (u or \"\").replace(\"µ\",\"u\").replace(\"μ\",\"u\").replace(\"°\",\"deg\").replace(\"º\",\"deg\").strip()\n",
        "def to_canonical(val_str, unit_str):\n",
        "    try:\n",
        "        q = Q_(float(str(val_str).strip()), _clean_unit(unit_str))\n",
        "        qb = q.to_base_units()\n",
        "        return qb.magnitude, str(qb.units)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "_RID_ID_RE = re.compile(r\"\\b((?:EV|T|F|GR)\\.\\d+(?:\\.\\d+)*[a-z]?)\\b\", flags=re.I)\n",
        "\n",
        "def _normalize_rule_id(raw):\n",
        "    s = re.sub(r'\\s+', '', str(raw))\n",
        "    m = re.match(r'^(EV|T|F|GR)\\.(\\d+(?:\\.\\d+)*)([a-zA-Z]?)$', s, flags=re.I)\n",
        "    if not m: return None\n",
        "    return f\"{m.group(1).upper()}.{m.group(2)}{m.group(3).lower()}\"\n",
        "\n",
        "def explicit_rule_from_question(question, rule_chunks):\n",
        "    m = _RID_ID_RE.search(str(question) or \"\")\n",
        "    if not m: return None\n",
        "    rid = _normalize_rule_id(m.group(1))\n",
        "    if not rid: return None\n",
        "    if rid in rule_chunks: return rid\n",
        "    pm = re.match(r'^(EV|T|F|GR)\\.(\\d+(?:\\.\\d+)*)([a-z])$', rid)\n",
        "    if pm:\n",
        "        parent = f\"{pm.group(1)}.{pm.group(2)}\"\n",
        "        if parent in rule_chunks: return parent\n",
        "    best = process.extractOne(rid, list(rule_chunks.keys()), score_cutoff=85)\n",
        "    return best[0] if best else None"
      ],
      "metadata": {
        "id": "AkXvFtodhJ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule_ids   = list(rule_chunks.keys())\n",
        "rule_texts = [f\"{rid}: {rule_chunks[rid]}\" for rid in rule_ids]\n",
        "\n",
        "embedder   = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "dense_emb  = embedder.encode(rule_texts, normalize_embeddings=True, convert_to_numpy=True)\n",
        "faiss_index= faiss.IndexFlatIP(dense_emb.shape[1]); faiss_index.add(dense_emb)\n",
        "bm25       = BM25Okapi([t.split() for t in rule_texts])\n",
        "\n",
        "def retrieve_rules_hybrid(query, top_k=4):\n",
        "    qv = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)\n",
        "    D, I = faiss_index.search(qv, 30)\n",
        "    dense_rank = {int(i): r for r,i in enumerate(I[0])}\n",
        "    bm = bm25.get_top_n(query.split(), list(range(len(rule_texts))), n=30)\n",
        "    bm25_rank = {doc_id: r for r, doc_id in enumerate(bm)}\n",
        "    scores = {}\n",
        "    for ranking in (dense_rank, bm25_rank):\n",
        "        for doc_id, r in ranking.items():\n",
        "            scores[doc_id] = scores.get(doc_id, 0) + 1.0/(60 + r)\n",
        "    picked = []\n",
        "    for doc_id, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        rid = rule_ids[doc_id]\n",
        "        picked.append((rid, rule_chunks[rid]))\n",
        "        if len(picked) >= top_k: break\n",
        "    return picked"
      ],
      "metadata": {
        "id": "oQ5a_-3LhO7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pint import UnitRegistry\n",
        "ureg = UnitRegistry()\n",
        "ureg.define('lbf = pound_force')\n",
        "def normalize_units_pint(text):\n",
        "    def convert_match(m):\n",
        "        try:\n",
        "            value = float(m.group(1))\n",
        "            unit = m.group(2)\n",
        "            qty = value * ureg(unit)\n",
        "            if qty.check('[length]'):\n",
        "                return f\"{qty.to(ureg.mm).magnitude:.2f} mm\"\n",
        "            elif qty.check('[pressure]'):\n",
        "                return f\"{qty.to(ureg.MPa).magnitude:.2f} MPa\"\n",
        "            elif qty.check('[force]'):\n",
        "                return f\"{qty.to(ureg.N).magnitude:.2f} N\"\n",
        "            elif qty.check('[torque]'):\n",
        "                return f\"{qty.to(ureg.N*ureg.meter).magnitude:.2f} Nm\"\n",
        "            elif qty.check('[mass]'):\n",
        "                return f\"{qty.to(ureg.kg).magnitude:.2f} kg\"\n",
        "            elif qty.check('[temperature]'):\n",
        "                return f\"{qty.to(ureg.degC).magnitude:.1f} °C\"\n",
        "            else:\n",
        "                return f\"{value} {unit}\"\n",
        "        except Exception:\n",
        "            return m.group(0)\n",
        "\n",
        "    return re.sub(r'(\\d+(?:\\.\\d+)?)\\s*([a-zA-Z°]+)', convert_match, text)\n",
        "_reader = easyocr.Reader(['en'], gpu=False)\n",
        "def ocr_image(img_path: Path) -> str:\n",
        "    img = cv2.imread(str(img_path))\n",
        "    if img is None:\n",
        "        return \"\"\n",
        "    text = \" \".join(_reader.readtext(img, detail=0, paragraph=True))\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'(\\d)\\s*m\\s*m\\b', r'\\1 mm', text, flags=re.I)\n",
        "    text = normalize_units_pint(text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "m36oaZ_dnxLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANSWER_MODEL = \"gpt-4o-mini\"\n",
        "def call_answer_llm(question, ocr_text, rules_block):\n",
        "    prompt = f\"\"\"You are a technical evaluator for FSAE.\n",
        "Question: {question}\n",
        "Image OCR: {ocr_text}\n",
        "\n",
        "Rule(s):\n",
        "{rules_block}\n",
        "\n",
        "Task:\n",
        "1) Compare OCR values against rule thresholds clearly.\n",
        "2) Answer Yes/No, or INS.\n",
        "3) Write explanation with explicit numeric comparison (e.g., \"Measured 25 mm < required 30 mm → fails\").\n",
        "\n",
        "\n",
        "Respond ONLY in this exact format:\n",
        "Explanation: <one or two sentences citing the compared values and rule threshold>\n",
        "Answer: Yes/No/INSUFFICIENT RULE EVIDENCE\n",
        "\"\"\"\n",
        "    resp = client.responses.create(\n",
        "        model=ANSWER_MODEL,\n",
        "        input=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    out = resp.output_text.strip()\n",
        "    m_reason = re.search(r\"Explanation:\\s*(.+)\", out)\n",
        "    m_final  = re.search(r\"Answer:\\s*([A-Za-z ]+)\", out)\n",
        "    reasoning = m_reason.group(1).strip() if m_reason else \"\"\n",
        "    final_ans = m_final.group(1).strip() if m_final else out\n",
        "\n",
        "    fa = str(final_ans).strip().lower()\n",
        "    if \"insufficient\" in fa:\n",
        "        final_ans = \"INSUFFICIENT RULE EVIDENCE\"\n",
        "    elif fa.startswith(\"y\") or \" yes \" in f\" {fa} \":\n",
        "        final_ans = \"Yes\"\n",
        "    elif fa.startswith(\"n\") or \" no \" in f\" {fa} \":\n",
        "        final_ans = \"No\"\n",
        "    else:\n",
        "        final_ans = \"INSUFFICIENT RULE EVIDENCE\"\n",
        "    return reasoning, final_ans"
      ],
      "metadata": {
        "id": "EiXlXw3PhTmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPK = 4\n",
        "def build_rules_block(question: str) -> str:\n",
        "    rid = explicit_rule_from_question(question, rule_chunks)\n",
        "    if rid:\n",
        "        pairs = [(rid, rule_chunks[rid])]\n",
        "    else:\n",
        "        pairs = retrieve_rules_hybrid(question, top_k=TOPK)\n",
        "    return \"\\n\\n\".join([f\"{rid}: {txt}\" for rid, txt in pairs])\n",
        "\n",
        "rows = []\n",
        "for idx, r in enumerate(df.itertuples(index=False), 1):\n",
        "    img_name = str(r.image).strip()\n",
        "    img_path = IMG_DIR / img_name\n",
        "    q = str(r.question)\n",
        "\n",
        "    print(f\"[{idx}/{len(df)}] Processing {img_name} …\")\n",
        "\n",
        "    ocr_txt = ocr_image(img_path)\n",
        "    rules_blk = build_rules_block(q)\n",
        "    expl, ans = call_answer_llm(q, ocr_txt, rules_blk)\n",
        "    rows.append({\n",
        "        \"image\": img_name,\n",
        "        \"model_prediction\": ans,\n",
        "        \"reasoning\": expl\n",
        "    })\n",
        "\n",
        "PRED = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "r9qroVReepe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "EVAL_DIR = Path(\"/content/results\")\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FP_GT_CSV = CSV_PATH\n",
        "\n",
        "if not Path(\"/content/design_qa\").exists():\n",
        "    subprocess.run([\"git\",\"clone\",\"-q\",\"https://github.com/anniedoris/design_qa.git\",\"/content/design_qa\"], check=True)\n",
        "sys.path.append(\"/content/design_qa\")\n",
        "subprocess.run([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"rouge\",\"nltk\"], check=True)\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "\n",
        "GT = pd.read_csv(FP_GT_CSV).copy()\n",
        "\n",
        "for col in [\"image\",\"answer\",\"label\",\"gt\",\"expected\",\"GroundTruth\",\"question\"]:\n",
        "    if col in GT.columns:\n",
        "        GT[col] = GT[col].astype(str)\n",
        "\n",
        "for col in [\"image\",\"model_prediction\",\"reasoning\"]:\n",
        "    if col in PRED.columns:\n",
        "        PRED[col] = PRED[col].astype(str)\n",
        "from pathlib import Path as _P\n",
        "def _norm_png(x):\n",
        "    s = \"\" if pd.isna(x) else str(x).strip()\n",
        "    s = _P(s).name\n",
        "    if not s.lower().endswith(\".png\"):\n",
        "        s = f\"{s}.png\"\n",
        "    return s.lower()\n",
        "\n",
        "GT[\"image_norm\"]   = GT.get(\"image\", \"\").apply(_norm_png)\n",
        "PRED[\"image_norm\"] = PRED.get(\"image\", \"\").apply(_norm_png)\n",
        "if \"ground_truth\" not in GT.columns:\n",
        "    for c in [\"answer\",\"label\",\"gt\",\"expected\",\"GroundTruth\"]:\n",
        "        if c in GT.columns:\n",
        "            GT = GT.rename(columns={c:\"ground_truth\"})\n",
        "            break\n",
        "J = GT.merge(PRED, on=\"image_norm\", how=\"inner\", suffixes=(\"_gt\",\"_pred\"))\n",
        "missing = len(GT) - len(J)\n",
        "if missing > 0:\n",
        "    print(f\"[WARN] {missing} GT rows had no matching prediction by image name.\")\n",
        "YES, NO, INS = \"Yes\", \"No\", \"INSUFFICIENT RULE EVIDENCE\"\n",
        "\n",
        "def _norm_truth(s: str) -> str:\n",
        "    t = (s or \"\").strip().lower()\n",
        "    if t.startswith(\"y\"): return YES\n",
        "    if t.startswith(\"n\"): return NO\n",
        "    if \"insufficient\" in t: return INS\n",
        "    return INS\n",
        "\n",
        "def _norm_pred_to_yn_or_ins(s: str) -> str:\n",
        "    t = (s or \"\").strip().lower()\n",
        "    if t.startswith(\"y\"): return YES\n",
        "    if t.startswith(\"n\"): return NO\n",
        "    if \"insufficient\" in t: return INS\n",
        "    return INS\n",
        "\n",
        "def _compose_fp_pred(row) -> str:\n",
        "    reasoning = str(row.get(\"reasoning\",\"\") or \"\").strip()\n",
        "    yn = _norm_pred_to_yn_or_ins(str(row.get(\"model_prediction\",\"\") or \"\"))\n",
        "    expl = reasoning if reasoning else yn\n",
        "    return f\"Explanation: {expl} Answer: {yn}\"\n",
        "\n",
        "SAFE_EMPTY = \"__\"\n",
        "\n",
        "FP_EVAL = str(EVAL_DIR / \"functional_performance_eval_official.csv\")\n",
        "df_eval = pd.DataFrame({\n",
        "    \"ground_truth\": J[\"ground_truth\"].apply(lambda x: str(x) if pd.notna(x) else SAFE_EMPTY).map(_norm_truth),\n",
        "    \"model_prediction\": J.apply(_compose_fp_pred, axis=1).astype(str)\n",
        "})\n",
        "df_eval[\"explanation\"] = SAFE_EMPTY\n",
        "df_eval.to_csv(FP_EVAL, index=False, na_rep=SAFE_EMPTY)\n",
        "print(f\"[OK] Prepared official eval CSV → {FP_EVAL}\")\n",
        "\n",
        "FP_TWO_COL = str(EVAL_DIR / \"functional_performance_for_full_eval.csv\")\n",
        "df_two = pd.DataFrame({\n",
        "    \"ground_truth\": J[\"ground_truth\"].map(_norm_truth),\n",
        "    \"model_prediction\": J[\"model_prediction\"].map(_norm_pred_to_yn_or_ins)\n",
        "})\n",
        "df_two.to_csv(FP_TWO_COL, index=False)\n",
        "print(f\"[OK] Two-column GT vs Prediction → {FP_TWO_COL}\")\n",
        "from eval.metrics.metrics import eval_functional_performance_qa\n",
        "acc_macro, *_ = eval_functional_performance_qa(FP_EVAL)\n",
        "print(f\"\\nFunctional Performance — Accuracy (macro): {acc_macro:.3f}\")\n",
        "num_questions = len(J)\n",
        "with open(\"/content/functional_performance.txt\", \"w\") as f:\n",
        "    f.write(\"DesignQA Results\\n\")\n",
        "    f.write(\"Subset: Functional_Performance\\n\")\n",
        "    f.write(f\"Num Questions: {num_questions}\\n\")\n",
        "    f.write(f\"ACC: {acc_macro:.6f}\\n\")\n",
        "print(\"Score file → /content/functional_performance.txt\")"
      ],
      "metadata": {
        "id": "Zd5JXLv8-SQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}