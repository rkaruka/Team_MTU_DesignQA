{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v8wbS2ORPLav"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu sentence-transformers openai pymupdf\n",
        "!pip -q install rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nuxSZ_Z0vmW"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "#My Google collab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lesQPq85umD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY environment variable. Please set it before running.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpQpJCcVUvkw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Upload-FSAE PDF and Q&A CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "pdf_path = None\n",
        "csv_file = None\n",
        "\n",
        "for f in uploaded.keys():\n",
        "    if f.endswith(\".pdf\"):\n",
        "        pdf_path = f\n",
        "    elif f.endswith(\".csv\"):\n",
        "        csv_file = f\n",
        "\n",
        "if not pdf_path or not csv_file:\n",
        "    raise FileNotFoundError(\"Both a PDF and a CSV file are required.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7KSg_DylObK"
      },
      "outputs": [],
      "source": [
        "#import fitz\n",
        "##import re\n",
        "\n",
        "#doc = fitz.open(pdf_path)\n",
        "#raw_text = \"\\n\".join(page.get_text() for page in doc)\n",
        "\n",
        "#def clean_text(text):\n",
        "   # text = text.replace('\\xa0', ' ')\n",
        "    #text = re.sub(r'-\\n', '', text)\n",
        "    #text = re.sub(r'\\n+', ' ', text)\n",
        "    #text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    #return text.strip()\n",
        "#full_text = clean_text(raw_text)\n",
        "import fitz, re\n",
        "\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "HEADER_RES = [\n",
        "    re.compile(r'^\\s*Formula SAE.*Page\\s+\\d+\\s+of\\s+\\d+\\s*$', re.I),\n",
        "    re.compile(r'^\\s*Version\\s+\\d+(\\.\\d+)?\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s*$', re.I),\n",
        "    re.compile(r'^\\s*\\d+\\s*$'),\n",
        "]\n",
        "TOC_LINE_RE       = re.compile(r'.+\\.\\s?\\.\\s?\\.\\s+\\d+$')\n",
        "SECTION_BANNER_RE = re.compile(r'^[A-Z]{1,4}\\s*-\\s+.+$')\n",
        "\n",
        "\n",
        "RULE_ID_RE = re.compile(r'^\\s*[A-Z]{1,3}\\s*[-.]?\\s*\\d+(\\.\\d+)*\\b')\n",
        "UNIT_RE    = re.compile(r'\\b(\\d+(\\.\\d+)?)\\s*(mm|cm|in|inch(es)?)\\b', re.I)\n",
        "\n",
        "def extract_text_from_page(p):\n",
        "    return p.get_text(\"text\")\n",
        "\n",
        "def clean_lines(lines):\n",
        "    out = []\n",
        "    for ln in lines:\n",
        "        s = ln.rstrip().replace('\\xa0', ' ')\n",
        "        if any(rx.match(s) for rx in HEADER_RES):\n",
        "            continue\n",
        "        if TOC_LINE_RE.search(s):\n",
        "            continue\n",
        "        if SECTION_BANNER_RE.match(s):\n",
        "            continue\n",
        "        out.append(s)\n",
        "    return out\n",
        "\n",
        "def page_stats(txt: str):\n",
        "    lines = [l for l in txt.splitlines() if l.strip()]\n",
        "    n = len(lines)\n",
        "    toc_hits   = sum(1 for l in lines if TOC_LINE_RE.search(l))\n",
        "    rule_hits  = sum(1 for l in lines if RULE_ID_RE.match(l))\n",
        "    unit_hits  = sum(1 for l in lines if UNIT_RE.search(l))\n",
        "    return n, toc_hits, rule_hits, unit_hits\n",
        "\n",
        "def autodetect_start(doc, scan_first=20):\n",
        "    best = 0\n",
        "    for i in range(min(scan_first, len(doc))):\n",
        "        t = extract_text_from_page(doc[i])\n",
        "        n, toc_hits, rule_hits, unit_hits = page_stats(t)\n",
        "        if rule_hits >= 3 and toc_hits <= max(1, int(0.15 * n)):\n",
        "            return i\n",
        "        if rule_hits >= 1 and unit_hits >= 3:\n",
        "            best = i\n",
        "    return best\n",
        "\n",
        "auto_start = autodetect_start(doc)\n",
        "\n",
        "pages = []\n",
        "for i in range(auto_start, len(doc)):\n",
        "    t = extract_text_from_page(doc[i])\n",
        "    lines = clean_lines(t.splitlines())\n",
        "    if len(lines) > 0:\n",
        "        toc_like = sum(1 for l in lines if TOC_LINE_RE.search(l))\n",
        "        if toc_like > 0.3 * len(lines):\n",
        "            continue\n",
        "    pages.append(\"\\n\".join(lines))\n",
        "\n",
        "raw_text = \"\\n\".join(pages)\n",
        "print(f\"Start page (auto): {auto_start}  |  Collected text chars: {len(raw_text)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvpTilaelS_H"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = re.sub(r'-\\n', '', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "full_text = clean_text(raw_text)\n",
        "print(\"Cleaned text chars:\", len(full_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTn8941ljv6U"
      },
      "outputs": [],
      "source": [
        "import re, json\n",
        "RULE_HEAD_RE = re.compile(r'(?m)^(?P<rid>[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*)(?:[ \\t]+(?P<title>.+))?$')\n",
        "\n",
        "def parse_rules_from_text(text):\n",
        "    lines = text.splitlines()\n",
        "    rules = []\n",
        "    cur_id, buf = None, []\n",
        "\n",
        "    def flush():\n",
        "        nonlocal cur_id, buf, rules\n",
        "        if cur_id and buf:\n",
        "            content = \"\\n\".join(buf).strip()\n",
        "            if content:\n",
        "                if not content.startswith(cur_id):\n",
        "                    content = f\"{cur_id} \" + content\n",
        "                rules.append((cur_id, content))\n",
        "        cur_id, buf = None, []\n",
        "\n",
        "    for ln in lines:\n",
        "        s = ln.strip()\n",
        "        m = RULE_HEAD_RE.match(s)\n",
        "        if m:\n",
        "            flush()\n",
        "            cur_id = m.group(\"rid\").strip()\n",
        "            buf = [s]\n",
        "        else:\n",
        "            if cur_id:\n",
        "                buf.append(ln)\n",
        "\n",
        "    flush()\n",
        "    return rules\n",
        "\n",
        "rule_pairs = parse_rules_from_text(full_text)\n",
        "rule_chunks = {rid: txt for rid, txt in rule_pairs}\n",
        "\n",
        "with open(\"rule_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rule_chunks, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Total rules parsed:\", len(rule_chunks))\n",
        "print(\"Sample keys:\", list(sorted(rule_chunks.keys()))[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX2dEU5jnQpI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "rows = [{\"rule_id\": rid, \"chars\": len(txt), \"text\": txt} for rid, txt in rule_chunks.items()]\n",
        "df_rules = pd.DataFrame(rows).sort_values([\"rule_id\"]).reset_index(drop=True)\n",
        "df_rules.to_csv(\"rule_chunks_preview.csv\", index=False)\n",
        "print(\"Wrote: rule_chunks_preview.csv\")\n",
        "\n",
        "families = [\"GR\",\"AD\",\"DR\",\"V\",\"F\",\"T\",\"EV\",\"ES\",\"IC\",\"ICV\"]\n",
        "for fam in families:\n",
        "    cnt = (df_rules[\"rule_id\"].str.startswith(fam + \".\")).sum()\n",
        "    print(f\"{fam}: {cnt} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVq8T6daVAwP"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import numpy as np\n",
        "import re\n",
        "import faiss\n",
        "\n",
        "rule_ids = list(rule_chunks.keys())\n",
        "rule_texts = [rule_chunks[k] for k in rule_ids]\n",
        "\n",
        "def tok(s):\n",
        "    return re.findall(r\"[A-Za-z0-9\\.]+\", s.lower())\n",
        "\n",
        "bm25 = BM25Okapi([tok(t) for t in rule_texts])\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(rule_texts, normalize_embeddings=True, convert_to_numpy=True).astype(\"float32\")\n",
        "\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "ce_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "print(f\"FAISS index built with {len(rule_texts)} rules.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOYMcQJaXMON"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "if \"ground_truth\" in df.columns:\n",
        "    df = df.rename(columns={\"ground_truth\": \"answer\"})\n",
        "\n",
        "if \"id\" not in df.columns:\n",
        "    df[\"id\"] = [f\"ret_{i+1:03}\" for i in range(len(df))]\n",
        "\n",
        "df = df[[\"id\", \"question\", \"answer\"]]\n",
        "qa_data = df.to_dict(orient=\"records\")\n",
        "\n",
        "print(f\"Loaded {len(qa_data)} QA pairs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p66N0jHVD5W"
      },
      "outputs": [],
      "source": [
        "def extract_rule_id(question):\n",
        "    match = re.search(r'([A-Z]+\\.\\d+(?:\\.\\d+)*)', question)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def retrieve_chunks(question, model, index, rule_ids, rule_texts, top_k=5):\n",
        "    q_embedding = model.encode([question], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = index.search(q_embedding, top_k)\n",
        "    return [(rule_ids[i], rule_texts[i]) for i in I[0]]\n",
        "\n",
        "def build_prompt(question, retrieved_texts):\n",
        "    context = \"\\n\".join(retrieved_texts)\n",
        "    return f\"\"\"You are an expert in the FSAE competition rules.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Retrieved Rules:\n",
        "{context}\n",
        "\n",
        "Task:\n",
        "- Respond ONLY with the exact rule text as shown in the retrieved rules.\n",
        "- Do not paraphrase, summarize, reformat, or add any extra words.\n",
        "- Keep punctuation, capitalization, and spacing exactly as in the retrieved rule text.\n",
        "- If multiple rules are shown, return only the one that exactly matches the requested rule number.\n",
        "- If no matching rule exists, respond with exactly: No applicable rule found.\n",
        "\"\"\"\n",
        "\n",
        "def get_gpt4_answer(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr9V7x6zbg--"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "sampled_qa_data = qa_data\n",
        "\n",
        "for idx, qa in enumerate(sampled_qa_data):\n",
        "    try:\n",
        "        question = qa[\"question\"]\n",
        "        qa_id = qa[\"id\"]\n",
        "        ground_truth = qa[\"answer\"]\n",
        "\n",
        "        rule_id = extract_rule_id(question)\n",
        "        if rule_id and rule_id in rule_chunks:\n",
        "            retrieved = [(rule_id, f\"{rule_id} {rule_chunks[rule_id]}\")]\n",
        "        else:\n",
        "            retrieved = retrieve_chunks(question, model, index, rule_ids, rule_texts, top_k=1)\n",
        "\n",
        "        prompt = build_prompt(question, [text for _, text in retrieved])\n",
        "        answer = get_gpt4_answer(prompt)\n",
        "\n",
        "        results.append({\n",
        "            \"id\": qa_id,\n",
        "            \"question\": question,\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"prediction\": answer\n",
        "        })\n",
        "\n",
        "        print(f\"{qa_id} \")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {qa_id}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulEH3ihcFnMU"
      },
      "outputs": [],
      "source": [
        "import os, pandas as pd, importlib.util\n",
        "\n",
        "!rm -rf design_qa\n",
        "!git clone -q https://github.com/anniedoris/design_qa.git\n",
        "!pip -q install rouge nltk\n",
        "\n",
        "repo_dir = os.path.abspath(\"design_qa\")\n",
        "metrics_path = os.path.join(repo_dir, \"eval\", \"metrics\", \"metrics.py\")\n",
        "spec = importlib.util.spec_from_file_location(\"dq_metrics\", metrics_path)\n",
        "dq_metrics = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(dq_metrics)\n",
        "normalize_answer = dq_metrics.normalize_answer\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results[\"ground_truth\"] = df_results[\"ground_truth\"].apply(lambda x: normalize_answer(str(x)))\n",
        "df_results[\"prediction\"] = df_results[\"prediction\"].apply(lambda x: normalize_answer(str(x)))\n",
        "\n",
        "\n",
        "eval_csv = \"retrieval_evaluation.csv\"\n",
        "(df_results.rename(columns={\"prediction\": \"model_prediction\"})\n",
        "          .loc[:, [\"ground_truth\", \"model_prediction\"]]\n",
        ").to_csv(eval_csv, index=False)\n",
        "print(f\"{eval_csv} (columns: ground_truth, model_prediction)\")\n",
        "\n",
        "#scoring\n",
        "overall_f1, per_f1 = dq_metrics.eval_retrieval_qa(eval_csv)\n",
        "print(f\"Official Retrieval F1: {overall_f1:.6f} on {len(per_f1)} questions\")\n",
        "\n",
        "# results.txt ---\n",
        "with open(\"retrieval.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"DesignQA Results\\n\")\n",
        "    f.write(\"Subset: Retrieval\\n\")\n",
        "    f.write(f\"Num Questions: {len(per_f1)}\\n\")\n",
        "    f.write(f\"F1: {overall_f1:.6f}\\n\")\n",
        "print(\"results.txt\")\n",
        "\n",
        "# F1\n",
        "def bow_f1(pred, gt):\n",
        "    pt = dq_metrics.normalize_answer(str(pred)).split()\n",
        "    gt = dq_metrics.normalize_answer(str(gt)).split()\n",
        "    return dq_metrics.token_f1_score(pt, gt)\n",
        "\n",
        "df_detailed = (df_results\n",
        "               .rename(columns={\"prediction\": \"model_prediction\"})\n",
        "               .loc[:, [\"id\", \"question\", \"ground_truth\", \"model_prediction\"]]\n",
        "              ).copy()\n",
        "df_detailed[\"f1\"] = df_detailed.apply(lambda r: bow_f1(r[\"model_prediction\"], r[\"ground_truth\"]), axis=1)\n",
        "df_detailed.to_csv(\"retrieval_detailed_with_f1.csv\", index=False)\n",
        "print(\"retrieval_detailed_with_f1.csv (columns:id, question, ground_truth, model_prediction, f1)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
