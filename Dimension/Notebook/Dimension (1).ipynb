{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI7HO__BFTez"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai==1.* sentence-transformers faiss-cpu pandas tqdm pymupdf pillow requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, io, json, base64, importlib.util\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import requests, fitz  # PyMuPDF\n",
        "from PIL import Image, ImageEnhance, ImageOps"
      ],
      "metadata": {
        "id": "Ef44Sl5CFVE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "#My Google collab\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY environment variable. Please set it before running.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "5y4NzfWCFYWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "CSV_URL = \"https://raw.githubusercontent.com/anniedoris/design_qa/main/dataset/rule_compliance/rule_dimension_qa/detailed_context/rule_dimension_qa_detailed_context.csv\"\n",
        "df = pd.read_csv(CSV_URL)\n",
        "print(\"CSV rows:\", len(df))\n",
        "\n",
        "download_dir = Path(\"/content/detailed_context_images\")\n",
        "download_dir.mkdir(parents=True, exist_ok=True)\n",
        "BASE_IMG_URL = \"https://raw.githubusercontent.com/anniedoris/design_qa/main/dataset/rule_compliance/rule_dimension_qa/detailed_context/\"\n",
        "\n",
        "for fname in df['image'].unique():\n",
        "    p = download_dir / fname\n",
        "    if not p.exists():\n",
        "        r = requests.get(BASE_IMG_URL + fname, timeout=30)\n",
        "        if r.status_code == 200:\n",
        "            p.write_bytes(r.content)\n",
        "print(\"Images ready:\", len(list(download_dir.iterdir())))\n",
        "\n",
        "from google.colab import files\n",
        "print(\"Upload FSAE_Rules_2024_V1.pdf …\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "wTKC4eT3FY5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_page = 4\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "def extract_text_from_page(p): return p.get_text(\"text\")\n",
        "\n",
        "HEADER_RES = [\n",
        "    re.compile(r'^\\s*Formula SAE.*Page\\s+\\d+\\s+of\\s+\\d+\\s*$', re.I),\n",
        "    re.compile(r'^\\s*Version\\s+\\d+(\\.\\d+)?\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s*$', re.I),\n",
        "    re.compile(r'^\\s*\\d+\\s*$', re.I),\n",
        "]\n",
        "TOC_LINE_RE = re.compile(r'.+\\.\\s?\\.\\s?\\.\\s+\\d+$')\n",
        "SECTION_BANNER_RE = re.compile(r'^[A-Z]{1,4}\\s*-\\s+.+$')\n",
        "\n",
        "def clean_lines(lines):\n",
        "    out = []\n",
        "    for ln in lines:\n",
        "        s = ln.rstrip().replace('\\xa0',' ')\n",
        "        if any(rx.match(s) for rx in HEADER_RES): continue\n",
        "        if TOC_LINE_RE.search(s): continue\n",
        "        if SECTION_BANNER_RE.match(s): continue\n",
        "        out.append(s)\n",
        "    return out\n",
        "\n",
        "pages = []\n",
        "for i in range(start_page, len(doc)):\n",
        "    lines = extract_text_from_page(doc[i]).splitlines()\n",
        "    pages.append(\"\\n\".join(clean_lines(lines)))\n",
        "\n",
        "raw_text = \"\\n\".join(pages)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\xa0',' ')\n",
        "    text = re.sub(r'-\\n','', text)\n",
        "    text = re.sub(r'\\n{3,}','\\n\\n', text)\n",
        "    text = re.sub(r'[ \\t]+',' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "full_text = clean_text(raw_text)\n",
        "\n",
        "# --- Parse rules ---\n",
        "RULE_HEAD_RE = re.compile(r'(?m)^(?P<rid>[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*)(?:[ \\t]+(?P<title>.+))?$')\n",
        "\n",
        "def parse_rules_from_text(text):\n",
        "    lines = text.splitlines()\n",
        "    rules, cur_id, buf = [], None, []\n",
        "\n",
        "    def flush():\n",
        "        nonlocal cur_id, buf, rules\n",
        "        if cur_id and buf:\n",
        "            content = \"\\n\".join(buf).strip()\n",
        "            if content and not content.startswith(cur_id):\n",
        "                content = f\"{cur_id} \" + content\n",
        "            rules.append((cur_id, content))\n",
        "        cur_id, buf = None, []\n",
        "\n",
        "    for ln in lines:\n",
        "        s = ln.strip()\n",
        "        m = RULE_HEAD_RE.match(s)\n",
        "        if m:\n",
        "            flush()\n",
        "            cur_id = m.group(\"rid\").strip()\n",
        "            buf = [s]\n",
        "        elif cur_id:\n",
        "            buf.append(ln)\n",
        "    flush()\n",
        "    return rules\n",
        "\n",
        "rule_pairs = parse_rules_from_text(full_text)\n",
        "rule_chunks = {rid: txt for rid, txt in rule_pairs}\n",
        "print(\"Total rules parsed:\", len(rule_chunks))\n",
        "\n",
        "with open(\"/content/rule_chunks.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(rule_chunks, f, indent=2, ensure_ascii=False)\n",
        "pd.DataFrame([{\"rule_id\":k,\"chars\":len(v),\"text\":v} for k,v in rule_chunks.items()])\\\n",
        "  .sort_values(\"rule_id\").to_csv(\"/content/rule_chunks_preview.csv\", index=False)\n",
        "print(\"Wrote rule_chunks.json + rule_chunks_preview.csv\")"
      ],
      "metadata": {
        "id": "T86y9mZ9Fcah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "rule_ids   = list(rule_chunks.keys())\n",
        "rule_texts = [f\"{rid}: {rule_chunks[rid]}\" for rid in rule_ids]\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "rule_emb = embedder.encode(rule_texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "index = faiss.IndexFlatIP(rule_emb.shape[1])\n",
        "index.add(rule_emb)\n",
        "\n",
        "def retrieve_rule_candidates(query, top_k=5):\n",
        "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(q, top_k)\n",
        "    return [(rule_ids[i], rule_texts[i]) for i in I[0]]\n",
        "\n",
        "def retrieve_exact_rule_or_fallback(question, top_k=5):\n",
        "    ids_in_q = re.findall(r'[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*(?:[a-z])?', question)\n",
        "    primary = None\n",
        "    for cand in ids_in_q:\n",
        "        if cand in rule_chunks and len(rule_chunks[cand]) > 40:\n",
        "            primary = (cand, f\"{cand}: {rule_chunks[cand]}\")\n",
        "            break\n",
        "    cands = retrieve_rule_candidates(question, top_k=top_k)\n",
        "    if primary:\n",
        "        cands = [x for x in cands if x[0] != primary[0]]\n",
        "        cands.insert(0, primary)\n",
        "    return cands[:top_k]\n",
        "\n",
        "def load_image(path, max_side=2400):\n",
        "    im = Image.open(path)\n",
        "    im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
        "    w, h = im.size\n",
        "    if max(w, h) > max_side:\n",
        "        scale = max_side / max(w, h)\n",
        "        im = im.resize((int(w*scale), int(h*scale)), Image.LANCZOS)\n",
        "    return im\n",
        "\n",
        "def enhance_for_reading(img: Image.Image, upscale=1.5):\n",
        "    w, h = img.size\n",
        "    img2 = img.resize((int(w*upscale), int(h*upscale)), Image.LANCZOS)\n",
        "    img2 = ImageEnhance.Color(img2).enhance(0.0)\n",
        "    img2 = ImageEnhance.Contrast(img2).enhance(1.6)\n",
        "    img2 = ImageEnhance.Sharpness(img2).enhance(1.8)\n",
        "    return img2\n",
        "\n",
        "def make_top_bar_crop(img: Image.Image, frac=0.30):\n",
        "    w, h = img.size\n",
        "    return img.crop((0, 0, w, int(h*frac)))\n",
        "\n",
        "def to_b64(img, fmt=\"JPEG\", quality=95):\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=fmt, quality=quality)\n",
        "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "def robust_json_extract(text):\n",
        "    m = re.search(r'\\{.*\\}', str(text), re.S)\n",
        "    if m:\n",
        "        try: return json.loads(m.group(0))\n",
        "        except: pass\n",
        "    mv = re.search(r'(\\d+(?:\\.\\d+)?)\\s*mm', str(text))\n",
        "    return {\"answer\":\"PARSE_ERROR\",\"measured_value\": (mv.group(0) if mv else None),\n",
        "            \"rule_id\":None,\"explanation\":str(text)[:500]}\n",
        "\n",
        "def parse_mm(s):\n",
        "    if s is None: return None\n",
        "    mm = re.search(r'(\\d+(?:\\.\\d+)?)\\s*mm', str(s).replace(\",\", \"\"))\n",
        "    return float(mm.group(1)) if mm else None\n",
        "\n",
        "def parse_threshold(rule_text):\n",
        "    t = rule_text.lower().replace(\",\", \"\")\n",
        "    nums = list(re.finditer(r'(\\d+(?:\\.\\d+)?)\\s*mm', t))\n",
        "    if not nums: return None, None\n",
        "    def sense(m):\n",
        "        pos = m.start(); w = t[max(0,pos-100):pos+100]\n",
        "        if any(k in w for k in [\"minimum\",\"at least\",\"no less\",\"≥\",\">=\"]): return (\">=\", float(m.group(1)))\n",
        "        if any(k in w for k in [\"maximum\",\"at most\",\"no more\",\"≤\",\"<=\"]):   return (\"<=\", float(m.group(1)))\n",
        "        if any(k in w for k in [\"exactly\",\"equal to\",\"equals\",\"must be\"]):  return (\"==\", float(m.group(1)))\n",
        "        return None\n",
        "    for m in nums:\n",
        "        s = sense(m)\n",
        "        if s: return s\n",
        "    return (\">=\", float(nums[0].group(1)))\n",
        "\n",
        "def radius_to_diameter_if_needed(text_near_value, measured_mm):\n",
        "    return measured_mm * 2.0 if (\"radius\" in (text_near_value or \"\").lower() and \"diameter\" not in (text_near_value or \"\").lower() and measured_mm is not None) else measured_mm\n",
        "\n",
        "def decide_answer(rule_text, measured_value_str, text_for_hints=None, tol=0.5):\n",
        "    op, thr = parse_threshold(rule_text)\n",
        "    meas = parse_mm(measured_value_str)\n",
        "    meas = radius_to_diameter_if_needed(text_for_hints, meas)\n",
        "    if op is None or thr is None or meas is None:\n",
        "        return \"CANNOT BE DETERMINED\", {\"op\":op,\"thr\":thr,\"meas\":meas,\"why\":\"missing data\"}\n",
        "    if op == \">=\": ans = \"Yes\" if meas >= thr - tol else \"No\"\n",
        "    elif op == \"<=\": ans = \"Yes\" if meas <= thr + tol else \"No\"\n",
        "    elif op == \"==\": ans = \"Yes\" if abs(meas - thr) <= tol else \"No\"\n",
        "    else: ans = \"CANNOT BE DETERMINED\"\n",
        "    return ans, {\"op\":op,\"thr\":thr,\"meas\":meas}"
      ],
      "metadata": {
        "id": "wlpOi2vnFls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"You are an FSAE rule compliance checker. Use ONLY the engineering drawing(s) and the rule text.\"\n",
        "\n",
        "def ask_model(question, image_b64_list, rules_joined, model=\"gpt-4o\"):\n",
        "    user_prompt = f\"\"\"\n",
        "Rule(s):\n",
        "{rules_joined}\n",
        "\n",
        "Task:\n",
        "1) Identify EXACTLY the dimension relevant to the rule (name it).\n",
        "2) Quote the dimension text exactly as shown on the drawing.\n",
        "3) If the drawing shows radius but the rule uses diameter, compute diameter = 2 × radius.\n",
        "4) If the question mentions a scale bar, use it and state the pixel→mm conversion.\n",
        "5) Compare the measured value with the rule requirement and decide compliance.\n",
        "\n",
        "Output JSON ONLY:\n",
        "{{\n",
        "  \"answer\": \"Yes\" | \"No\" | \"CANNOT BE DETERMINED\",\n",
        "  \"measured_value\": \"<number> mm\",\n",
        "  \"rule_id\": \"<best matching rule id>\",\n",
        "  \"explanation\": \"<1–3 sentence; include the quoted dimension label>\"\n",
        "}}\n",
        "\"\"\"\n",
        "    content = [{\"type\":\"text\",\"text\": user_prompt}]\n",
        "    for b64 in image_b64_list:\n",
        "        content.append({\"type\":\"image_url\",\"image_url\":{\"url\":f\"data:image/jpeg;base64,{b64}\",\"detail\":\"high\"}})\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model, temperature=0, max_tokens=450,\n",
        "        messages=[{\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
        "                  {\"role\":\"user\",\"content\": content}]\n",
        "    )\n",
        "    raw = resp.choices[0].message.content or \"\"\n",
        "    return robust_json_extract(raw), raw\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    q = row['question']\n",
        "    img_file = row['image']\n",
        "    gt = str(row['ground_truth']).strip().lower()\n",
        "\n",
        "    p = download_dir / img_file\n",
        "    if not p.exists():\n",
        "        print(\"Missing:\", p); continue\n",
        "\n",
        "    img = load_image(p)\n",
        "    img_enh = enhance_for_reading(img, upscale=1.5)\n",
        "    images_b64 = [to_b64(img), to_b64(img_enh)]\n",
        "    if \"scale bar\" in q.lower():\n",
        "        images_b64.append(to_b64(make_top_bar_crop(img_enh, frac=0.32)))\n",
        "\n",
        "    retrieved = retrieve_exact_rule_or_fallback(q, top_k=5)\n",
        "    rules_joined = \"\\n\".join([rt for rid, rt in retrieved])[:1400]\n",
        "\n",
        "    mdict, raw_text = ask_model(q, images_b64, rules_joined, model=\"gpt-4o\")\n",
        "    final_answer, audit = decide_answer(rules_joined, mdict.get(\"measured_value\"), mdict.get(\"explanation\",\"\"))\n",
        "\n",
        "    model_prediction_text = f\"Explanation: {mdict.get('explanation','').strip()} Answer: {final_answer}\"\n",
        "\n",
        "    results.append({\n",
        "        \"idx\": idx, \"question\": q, \"image\": img_file,\n",
        "        \"ground_truth\": gt, \"retrieved_rule_top0\": retrieved[0][1] if retrieved else \"\",\n",
        "        \"raw_model_text\": raw_text[:1000], \"measured_value\": mdict.get(\"measured_value\"),\n",
        "        \"answer_model\": mdict.get(\"answer\"), \"answer_final\": final_answer,\n",
        "        \"audit\": audit, \"model_prediction\": model_prediction_text\n",
        "    })\n"
      ],
      "metadata": {
        "id": "5m6FuvqtFmPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "res_df.to_csv(\"/content/dimension_full_predictions.csv\", index=False)\n",
        "print(\"Wrote: /content/dimension_full_predictions.csv\")\n"
      ],
      "metadata": {
        "id": "CwjYMWH4JhdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess, re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    PRED = res_df.copy()\n",
        "except NameError:\n",
        "    PRED = pd.read_csv(\"/content/dimension_full_predictions.csv\").copy()\n",
        "\n",
        "if not Path(\"/content/design_qa\").exists():\n",
        "    subprocess.run(\n",
        "        [\"git\", \"clone\", \"-q\",\n",
        "         \"https://github.com/anniedoris/design_qa.git\",\n",
        "         \"/content/design_qa\"],\n",
        "        check=True\n",
        "    )\n",
        "sys.path.append(\"/content/design_qa\")\n",
        "\n",
        "\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge\", \"nltk\"], check=True)\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "GT = pd.read_csv(CSV_URL).copy()\n",
        "if \"idx\" not in PRED.columns:\n",
        "    PRED = PRED.reset_index().rename(columns={\"index\":\"idx\"})\n",
        "if \"idx\" not in GT.columns:\n",
        "    GT = GT.reset_index().rename(columns={\"index\":\"idx\"})\n",
        "\n",
        "J = GT.merge(PRED, on=\"idx\", how=\"inner\", suffixes=(\"_gt\", \"_pred\"))\n",
        "missing = len(GT) - len(J)\n",
        "if missing > 0:\n",
        "    print(f\"[WARN] {missing} GT rows had no matching prediction by idx.\")\n",
        "YES, NO, CBD = \"yes\", \"no\", \"cannot be determined\"\n",
        "\n",
        "def _norm_truth(s: str) -> str:\n",
        "    t = (s or \"\").strip().lower()\n",
        "    if t.startswith(\"y\"): return YES\n",
        "    if t.startswith(\"n\"): return NO\n",
        "    return CBD\n",
        "\n",
        "def _norm_pred(s: str) -> str:\n",
        "    t = (s or \"\").strip().lower()\n",
        "    m = re.search(r\"\\banswer\\s*:\\s*(yes|no|cannot be determined)\\b\", t, re.I)\n",
        "    if m: t = m.group(1).lower()\n",
        "    if \"cannot\" in t: return CBD\n",
        "    if t.startswith(\"y\"): return YES\n",
        "    if t.startswith(\"n\"): return NO\n",
        "    return CBD\n",
        "\n",
        "def pick_col(df, *candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    raise KeyError(f\"None of the expected columns found. Looked for: {candidates}\\nHave: {list(df.columns)}\")\n",
        "\n",
        "gt_col        = pick_col(J, \"ground_truth_gt\", \"ground_truth\", \"GroundTruth\", \"answer\", \"gt\", \"expected\")\n",
        "pred_col      = \"answer_final\" if \"answer_final\" in J.columns else pick_col(J, \"model_prediction_pred\", \"model_prediction\")\n",
        "raw_col       = \"raw_model_text\" if \"raw_model_text\" in J.columns else None\n",
        "dimtype_col   = pick_col(J, \"dimension_type_gt\", \"dimension_type\")\n",
        "expl_gt_col   = pick_col(J, \"explanation_gt\", \"explanation\")\n",
        "J[\"ground_truth_norm\"]   = J[gt_col].apply(_norm_truth)\n",
        "J[\"model_prediction_ok\"] = J[pred_col].apply(_norm_pred)\n",
        "\n",
        "EVAL_DIR   = Path(\"/content/results\"); EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DIM_EVAL   = str(EVAL_DIR / \"dimension_eval_official.csv\")\n",
        "DIM_TWOCOL = str(EVAL_DIR / \"dimension_for_full_eval.csv\")\n",
        "\n",
        "# Official-style CSV\n",
        "df_eval = pd.DataFrame({\n",
        "    \"ground_truth\":    J[\"ground_truth_norm\"],\n",
        "    \"model_prediction\": J.apply(\n",
        "        lambda r: f\"Explanation: {str(r.get(raw_col,''))[:200]} \"\n",
        "                  f\"Answer: {r['model_prediction_ok']}\",\n",
        "        axis=1\n",
        "    ),\n",
        "    \"dimension_type\":  J[dimtype_col],\n",
        "    \"explanation\":     J[expl_gt_col],\n",
        "})\n",
        "df_eval.to_csv(DIM_EVAL, index=False)\n",
        "print(f\"[OK] Prepared official eval CSV → {DIM_EVAL}\")\n",
        "pd.DataFrame({\n",
        "    \"ground_truth\":    J[\"ground_truth_norm\"],\n",
        "    \"model_prediction\": J[\"model_prediction_ok\"]\n",
        "}).to_csv(DIM_TWOCOL, index=False)\n",
        "print(f\"[OK] Two-column GT vs Prediction → {DIM_TWOCOL}\")\n",
        "\n",
        "from eval.metrics.metrics import eval_dimensions_qa\n",
        "acc_macro, direct_avg, scale_avg, *_ = eval_dimensions_qa(DIM_EVAL)\n",
        "print(\"\\nDimension\")\n",
        "print(f\"(ACC ↑)\\t\\t{acc_macro:.3f}\")\n",
        "\n",
        "with open(\"/content/dimension.txt\", \"w\") as f:\n",
        "    f.write(\"DesignQA Results\\n\")\n",
        "    f.write(\"Subset: Dimension\\n\")\n",
        "    f.write(f\"Num Questions: {len(J)}\\n\")\n",
        "    f.write(f\"ACC: {acc_macro:.6f}\\n\")\n",
        "print(\"Score file → /content/dimension.txt\")"
      ],
      "metadata": {
        "id": "pfSkClcRFpeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
