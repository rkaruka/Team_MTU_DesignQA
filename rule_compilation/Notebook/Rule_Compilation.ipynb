{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFqq7Js8q2hB"
      },
      "outputs": [],
      "source": [
        "!pip -q install pymupdf faiss-cpu sentence-transformers pandas numpy rank-bm25 openai rouge nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, math, datetime, subprocess, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "kIOPq6AI_J7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "#My Google collab\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY environment variable. Please set it before running.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "Z8jNTKOvZFSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "pdf_path, csv_file = None, None\n",
        "for f in uploaded.keys():\n",
        "    if f.lower().endswith(\".pdf\"):\n",
        "        pdf_path = f\n",
        "    elif f.lower().endswith(\".csv\"):\n",
        "        csv_file = f\n",
        "if not pdf_path or not csv_file:\n",
        "    raise FileNotFoundError(\"Upload the FSAE Rulebook PDF and the Rule Compilation CSV.\")\n",
        "\n",
        "print(\"PDF:\", pdf_path, \"| CSV:\", csv_file)"
      ],
      "metadata": {
        "id": "DXO4-7YDtQEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYek0c9zq8kF"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "start_page = 4\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "def extract_text_from_page(p):\n",
        "    return p.get_text(\"text\")\n",
        "\n",
        "HEADER_RES = [\n",
        "    re.compile(r'^\\s*Formula SAE.*Page\\s+\\d+\\s+of\\s+\\d+\\s*$', re.I),\n",
        "    re.compile(r'^\\s*Version\\s+\\d+(\\.\\d+)?\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s*$', re.I),\n",
        "    re.compile(r'^\\s*\\d+\\s*$', re.I),\n",
        "]\n",
        "TOC_LINE_RE = re.compile(r'.+\\.\\s?\\.\\s?\\.\\s+\\d+$')\n",
        "SECTION_BANNER_RE = re.compile(r'^[A-Z]{1,4}\\s*-\\s+.+$')\n",
        "\n",
        "def clean_lines(lines):\n",
        "    out = []\n",
        "    for ln in lines:\n",
        "        s = ln.rstrip().replace('\\xa0', ' ')\n",
        "        if any(rx.match(s) for rx in HEADER_RES): continue\n",
        "        if TOC_LINE_RE.search(s): continue\n",
        "        if SECTION_BANNER_RE.match(s): continue\n",
        "        out.append(s)\n",
        "    return out\n",
        "\n",
        "pages = []\n",
        "for i in range(start_page, len(doc)):\n",
        "    t = extract_text_from_page(doc[i])\n",
        "    lines = clean_lines(t.splitlines())\n",
        "    pages.append(\"\\n\".join(lines))\n",
        "\n",
        "raw_text = \"\\n\".join(pages)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = re.sub(r'-\\n', '', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "full_text = clean_text(raw_text)\n",
        "print(\"Cleaned text chars:\", len(full_text))\n",
        "RULE_HEAD_RE = re.compile(r'(?m)^(?P<rid>[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*)(?:[ \\t]+(?P<title>.+))?$')\n",
        "\n",
        "def parse_rules_from_text(text):\n",
        "    lines = text.splitlines()\n",
        "    rules, cur_id, buf = [], None, []\n",
        "    def flush():\n",
        "        nonlocal cur_id, buf, rules\n",
        "        if cur_id and buf:\n",
        "            content = \"\\n\".join(buf).strip()\n",
        "            if content:\n",
        "                if not content.startswith(cur_id):\n",
        "                    content = f\"{cur_id} \" + content\n",
        "                rules.append((cur_id, content))\n",
        "        cur_id, buf = None, []\n",
        "    for ln in lines:\n",
        "        s = ln.strip()\n",
        "        m = RULE_HEAD_RE.match(s)\n",
        "        if m:\n",
        "            flush(); cur_id = m.group(\"rid\").strip(); buf = [s]\n",
        "        else:\n",
        "            if cur_id: buf.append(ln)\n",
        "    flush()\n",
        "    return rules\n",
        "\n",
        "def stitch_parent_children(rule_pairs):\n",
        "    by_parent = defaultdict(list)\n",
        "    for rid, txt in rule_pairs:\n",
        "        parts = rid.split(\".\")\n",
        "        if len(parts) >= 2:\n",
        "            parent_lvl1 = \".\".join(parts[:2])  # e.g., \"T.7\"\n",
        "            by_parent[parent_lvl1].append((rid, txt))\n",
        "    def keyfn(x):\n",
        "        rid = x[0]; parts = rid.split(\".\")[1:]\n",
        "        return [int(p) if p.isdigit() else p for p in parts]\n",
        "    stitched = {}\n",
        "    for parent, children in by_parent.items():\n",
        "        children_sorted = sorted(children, key=keyfn)\n",
        "        stitched[parent] = \"\\n\".join(txt for _, txt in children_sorted)\n",
        "    return stitched\n",
        "\n",
        "rule_pairs = parse_rules_from_text(full_text)\n",
        "rule_chunks = {rid: txt for rid, txt in rule_pairs}\n",
        "rule_chunks.update(stitch_parent_children(rule_pairs))\n",
        "print(\"Total rule entries (with stitched parents):\", len(rule_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import faiss\n",
        "\n",
        "rule_ids   = list(rule_chunks.keys())\n",
        "rule_texts = [rule_chunks[k] for k in rule_ids]\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "emb = model.encode(rule_texts, show_progress_bar=True, convert_to_numpy=True).astype(\"float32\")\n",
        "emb = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "index = faiss.IndexFlatIP(emb.shape[1])\n",
        "index.add(emb)\n",
        "print(\"FAISS index size:\", index.ntotal)\n",
        "\n",
        "def tokenize(s): return re.findall(r\"[a-z0-9]+\", s.lower())\n",
        "bm25 = BM25Okapi([tokenize(t) for t in rule_texts])"
      ],
      "metadata": {
        "id": "Z4cIryJTHvzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = None\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    try:\n",
        "        client = OpenAI(\"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\")  # reads OPENAI_API_KEY\n",
        "        print(\"OpenAI client initialized.\")\n",
        "    except Exception as e:\n",
        "        print(\"OpenAI init failed; continuing without LLM:\", e)\n",
        "else:\n",
        "    print(\".\")"
      ],
      "metadata": {
        "id": "nXQVABdrQDC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RID_RE = re.compile(r'[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*')\n",
        "\n",
        "def extract_focus_phrase(q: str):\n",
        "    m = re.search(r'\"([^\"]+)\"', q) or re.search(r'`([^`]+)`', q)\n",
        "    return (m.group(1).strip() if m else q.strip())\n",
        "\n",
        "SYN = {\n",
        "    \"aerodynamics\": [\"aero\", \"wing\", \"splitter\", \"diffuser\", \"undertray\", \"endplate\", \"bodywork\"],\n",
        "    \"aerodynamic\":  [\"aero\", \"wing\", \"splitter\", \"diffuser\", \"undertray\", \"endplate\", \"bodywork\"],\n",
        "    \"impact attenuator\": [\"ia\", \"front impact\", \"energy absorber\", \"attenuator\"],\n",
        "    \"accumulator\": [\"battery pack\", \"hv\", \"shutdown\", \"ams\", \"tsal\"],\n",
        "    \"seat belt\": [\"harness\", \"restraint\", \"belts\"],\n",
        "    \"inspection\": [\"tilt\", \"brake test\", \"noise test\", \"scrutineering\"],\n",
        "}\n",
        "def build_cooccur_map(text, window=10, min_freq=5):\n",
        "    toks = tokenize(text)\n",
        "    pos = defaultdict(list)\n",
        "    for i,t in enumerate(toks): pos[t].append(i)\n",
        "    co = defaultdict(Counter)\n",
        "    for w, idxs in pos.items():\n",
        "        for i in idxs:\n",
        "            lo, hi = max(0, i-window), min(len(toks), i+window+1)\n",
        "            for j in range(lo, hi):\n",
        "                if j==i: continue\n",
        "                co[w][toks[j]] += 1\n",
        "    return {w:{t:c for t,c in cnt.items() if c>=min_freq} for w,cnt in co.items()}\n",
        "\n",
        "CO = build_cooccur_map(full_text, window=10, min_freq=5)\n",
        "\n",
        "def expand_focus(focus):\n",
        "    f = (focus or \"\").strip().lower()\n",
        "    extras = set(SYN.get(f, []))\n",
        "    extras.update([t for t,_c in Counter(CO.get(f, {})).most_common(6)])\n",
        "    extras = [e for e in extras if len(e)>2]\n",
        "    return focus if not extras else f\"{focus} \" + \" \".join(sorted(set(extras))[:8])\n",
        "\n",
        "SECTION_MAP = {\n",
        "    \"aero\": [\"T.\"], \"aerodynamic\": [\"T.\"], \"aerodynamics\": [\"T.\"],\n",
        "    \"wing\": [\"T.\"], \"splitter\": [\"T.\"], \"diffuser\": [\"T.\"], \"undertray\": [\"T.\"], \"endplate\": [\"T.\"], \"bodywork\": [\"T.\"],\n",
        "    \"electrical\": [\"EV.\"], \"accumulator\": [\"EV.\"], \"hv\": [\"EV.\"], \"shutdown\": [\"EV.\"], \"insulation\": [\"EV.\"], \"ams\": [\"EV.\"], \"tsal\": [\"EV.\"],\n",
        "    \"chassis\": [\"F.\"], \"frame\": [\"F.\"], \"impact attenuator\": [\"F.\"], \"attenuator\": [\"F.\"], \"node\": [\"F.\"], \"tube\": [\"F.\"],\n",
        "    \"inspection\": [\"IN.\"], \"tilt\": [\"IN.\"], \"brake\": [\"IN.\"], \"noise\": [\"IN.\"], \"scrutineering\": [\"IN.\"],\n",
        "    \"presentation\": [\"S.\"], \"cost\": [\"C.\"], \"design\": [\"DR.\"],\n",
        "    \"engine\": [\"IC.\"], \"combustion\": [\"IC.\"], \"intake\": [\"IC.\"],\n",
        "    \"electric\": [\"EV.\"],\n",
        "}\n",
        "\n",
        "def guess_sections_from_text(text):\n",
        "    t = text.lower()\n",
        "    sections = set()\n",
        "    for k, secs in SECTION_MAP.items():\n",
        "        if k in t: sections.update(secs)\n",
        "    if sections: sections.add(\"GR.\")\n",
        "    return sections\n",
        "\n",
        "def retrieve_hybrid(q, k_dense=140, k_bm25=140, rrf_k=60, w_dense=1.0, w_bm25=1.35):\n",
        "    qv = model.encode([q], convert_to_numpy=True).astype(\"float32\")\n",
        "    qv = qv / (np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)\n",
        "    D, I = index.search(qv, k_dense)\n",
        "    dense_rank = {rule_ids[i]: r for r, i in enumerate(I[0], start=1)}\n",
        "    scores = bm25.get_scores(tokenize(q))\n",
        "    top = np.argsort(scores)[-k_bm25:][::-1]\n",
        "    bm25_rank = {rule_ids[i]: r for r, i in enumerate(top, start=1)}\n",
        "    def rrf(rank): return 1.0/(rrf_k + rank)\n",
        "    fused, seen = [], set(dense_rank) | set(bm25_rank)\n",
        "    for rid in seen:\n",
        "        s = w_dense*rrf(dense_rank.get(rid, 10_000)) + w_bm25*rrf(bm25_rank.get(rid, 10_000))\n",
        "        fused.append((s, rid))\n",
        "    fused.sort(reverse=True)\n",
        "    return [(rid, rule_chunks[rid]) for s, rid in fused]\n",
        "\n",
        "# section gating\n",
        "def soft_section_filter(cands, allowed_secs, keep_top=80, spillover=20):\n",
        "    if not allowed_secs:\n",
        "        return cands[:keep_top]\n",
        "    in_sec  = [(rid, txt) for rid, txt in cands if rid.split(\".\")[0]+\".\" in allowed_secs]\n",
        "    out_sec = [(rid, txt) for rid, txt in cands if rid.split(\".\")[0]+\".\" not in allowed_secs]\n",
        "    return in_sec[:keep_top] + out_sec[:spillover]\n",
        "\n",
        "def first_line(rid):\n",
        "    return rule_chunks[rid].splitlines()[0][:120].replace(\"\\n\",\" \")\n",
        "def rerank_with_gpt_strict(query, candidate_ids, max_pick=12):\n",
        "    if client is None:\n",
        "        return candidate_ids[:max_pick]\n",
        "    prompt = f\"\"\"You are an FSAE rules expert.\n",
        "From the candidate list, return ONLY rule IDs, comma-separated. At most {max_pick}.\n",
        "No extra words.\n",
        "\n",
        "Query: {query}\n",
        "Candidates: {', '.join(candidate_ids)}\n",
        "Output:\"\"\"\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        raw = (resp.choices[0].message.content or \"\").strip()\n",
        "    except Exception as e:\n",
        "        return candidate_ids[:max_pick]\n",
        "    ids = [x.strip().rstrip(\".\") for x in re.split(r'[,\\s]+', raw)]\n",
        "    ids = [x for x in ids if RID_RE.fullmatch(x)]\n",
        "    if not ids:\n",
        "        ids = candidate_ids[:max_pick]\n",
        "    return ids[:max_pick]\n",
        "\n",
        "def expand_hierarchy_smart(picked_ids, cands, focus=None, per_parent_cap=3, total_cap=40):\n",
        "    pool_ids = {rid for rid, _ in cands}\n",
        "    focus_kw = set(tokenize(focus or \"\"))\n",
        "    def children_of(parent):\n",
        "        pref = parent + \".\"\n",
        "        return [rid for rid in pool_ids if rid.startswith(pref)]\n",
        "    final, seen = [], set()\n",
        "    for rid in picked_ids:\n",
        "        if rid not in seen:\n",
        "            final.append(rid); seen.add(rid)\n",
        "        kids = children_of(rid)\n",
        "        scored = []\n",
        "        for kid in kids:\n",
        "            text = rule_chunks.get(kid, \"\").lower()\n",
        "            hit = sum(1 for w in focus_kw if w and w in text)\n",
        "            scored.append((hit, kid))\n",
        "        scored.sort(reverse=True)\n",
        "        for _, kid in scored[:per_parent_cap]:\n",
        "            if kid not in seen:\n",
        "                final.append(kid); seen.add(kid)\n",
        "        if len(final) >= total_cap: break\n",
        "    return final[:total_cap]\n",
        "\n",
        "NEG = {\n",
        "    \"aerodynamic\": [\"presentation\", \"cost\", \"business\", \"design event\"],\n",
        "    \"aerodynamics\": [\"presentation\", \"cost\", \"business\", \"design event\"],\n",
        "}\n",
        "\n",
        "def post_filter_final(final_ids, focus_kw):\n",
        "    keep, neg = [], set()\n",
        "    for f in focus_kw:\n",
        "        neg.update(NEG.get(f, []))\n",
        "    for rid in final_ids:\n",
        "        txt = rule_chunks.get(rid, \"\").lower()\n",
        "        if any(w in txt for w in focus_kw) or len(rid.split(\".\")) <= 2:\n",
        "            if not any(n in txt for n in neg):\n",
        "                keep.append(rid)\n",
        "    return keep\n",
        "\n",
        "def normalize_ids(ids, cap=50):\n",
        "    out, seen = [], set()\n",
        "    for x in ids:\n",
        "        x = x.strip().rstrip(\".\")\n",
        "        if RID_RE.fullmatch(x) and x not in seen:\n",
        "            out.append(x); seen.add(x)\n",
        "        if len(out) >= cap: break\n",
        "    return out\n",
        "\n",
        "def natural_key(rid):\n",
        "    parts = rid.split(\".\")\n",
        "    out = [parts[0]]\n",
        "    for p in parts[1:]:\n",
        "        out.append(int(p) if p.isdigit() else p)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "Zx21tuGvKczV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = pd.read_csv(csv_file)\n",
        "compiled = []\n",
        "\n",
        "for i, row in qa_df.iterrows():\n",
        "    qid   = row.get(\"id\", f\"q_{i:04d}\")\n",
        "    q_raw = row[\"question\"]\n",
        "    gt    = row.get(\"ground_truth\",\"\")\n",
        "\n",
        "    focus_phrase = extract_focus_phrase(q_raw)\n",
        "    query_for_retrieval = expand_focus(focus_phrase)\n",
        "    allowed_secs = guess_sections_from_text(query_for_retrieval)\n",
        "\n",
        "    cands = retrieve_hybrid(query_for_retrieval, k_dense=140, k_bm25=140)\n",
        "    cands = soft_section_filter(cands, allowed_secs, keep_top=80, spillover=20)\n",
        "\n",
        "    candidate_ids = [rid for rid, _ in cands[:60]]\n",
        "    picked_ids = rerank_with_gpt_strict(query_for_retrieval, candidate_ids, max_pick=12)\n",
        "\n",
        "    final_ids = expand_hierarchy_smart(picked_ids, cands, focus=focus_phrase, per_parent_cap=3, total_cap=40)\n",
        "    focus_kw = set(tokenize(focus_phrase))\n",
        "    final_ids = post_filter_final(final_ids, focus_kw)\n",
        "    final_ids = normalize_ids(final_ids, cap=50)\n",
        "    final_ids = sorted(set(final_ids), key=natural_key)[:50]  # CHANGE: stable order\n",
        "\n",
        "    compiled.append({\n",
        "        \"id\": qid,\n",
        "        \"question\": q_raw,\n",
        "        \"focus_used\": focus_phrase,\n",
        "        \"ground_truth\": gt,\n",
        "        \"compiled_rule_ids\": \", \".join(final_ids)\n",
        "    })\n",
        "\n",
        "    if client is not None:\n",
        "        time.sleep(0.35)  # be polite to API\n",
        "\n",
        "out_df = pd.DataFrame(compiled)\n",
        "out_df.to_csv(\"compiled_rule_answers.csv\", index=False)\n",
        "print(\"Saved:\", \"compiled_rule_answers.csv\")"
      ],
      "metadata": {
        "id": "Tejf26haKls_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf design_qa\n",
        "!git clone -q https://github.com/anniedoris/design_qa.git\n",
        "repo_dir = os.path.abspath(\"design_qa\")\n",
        "metrics_path = os.path.join(repo_dir, \"eval\", \"metrics\", \"metrics.py\")\n",
        "\n",
        "import importlib.util\n",
        "spec = importlib.util.spec_from_file_location(\"dq_metrics\", metrics_path)\n",
        "dq_metrics = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(dq_metrics)\n",
        "\n",
        "df_eval = pd.DataFrame({\n",
        "    \"ground_truth\": out_df[\"ground_truth\"].astype(str).str.strip(),\n",
        "    \"model_prediction\": (out_df[\"compiled_rule_ids\"]\n",
        "                         .fillna(\"\")\n",
        "                         .apply(lambda s: \", \".join([x.strip() for x in str(s).split(\",\") if x.strip()])))\n",
        "})\n",
        "df_eval.to_csv(\"compilation_evaluation.csv\", index=False)\n",
        "print(\"Wrote:\", \"compilation_evaluation.csv\")\n",
        "\n",
        "overall_f1, per_f1 = dq_metrics.eval_compilation_qa(\"compilation_eval.csv\")\n",
        "print(f\"Compilation F1 (macro): {overall_f1:.6f} on {len(per_f1)} questions\")\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(os.path.join(\"results\", \"compilation.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"DesignQA Results\\n\")\n",
        "    f.write(\"Subset: Compilation\\n\")\n",
        "    f.write(f\"F1: {overall_f1:.6f}\\n\")\n",
        "    f.write(f\"Num Questions: {len(per_f1)}\\n\")\n",
        "\n",
        "    def git_sha():\n",
        "        try:\n",
        "            return subprocess.check_output([\"git\",\"rev-parse\",\"--short\",\"HEAD\"]).decode().strip()\n",
        "        except Exception:\n",
        "            return \"<git-sha>\"\n",
        "\n",
        "detail = out_df.copy()\n",
        "detail[\"f1\"] = per_f1\n",
        "detail.to_csv(\"compilation_detailed_with_f1.csv\", index=False)\n",
        "print(\"Wrote:\", \"results/compilation.txt and compilation_detailed_with_f1.csv\")"
      ],
      "metadata": {
        "id": "k8gdjgR3KxoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
