{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFqq7Js8q2hB"
      },
      "outputs": [],
      "source": [
        "!pip -q install pymupdf faiss-cpu sentence-transformers pandas numpy rank-bm25 openai rouge nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, math, datetime, subprocess, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "kIOPq6AI_J7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "#My Google collab\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY environment variable. Please set it before running.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "Z8jNTKOvZFSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "pdf_path, csv_file = None, None\n",
        "for f in uploaded.keys():\n",
        "    if f.lower().endswith(\".pdf\"):\n",
        "        pdf_path = f\n",
        "    elif f.lower().endswith(\".csv\"):\n",
        "        csv_file = f\n",
        "if not pdf_path or not csv_file:\n",
        "    raise FileNotFoundError(\"Upload the FSAE Rulebook PDF and the Rule Compilation CSV.\")\n",
        "\n",
        "print(\"PDF:\", pdf_path, \"| CSV:\", csv_file)"
      ],
      "metadata": {
        "id": "DXO4-7YDtQEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYek0c9zq8kF"
      },
      "outputs": [],
      "source": [
        "import fitz, re\n",
        "\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "HEADER_RES = [\n",
        "    re.compile(r'^\\s*Formula SAE.*Page\\s+\\d+\\s+of\\s+\\d+\\s*$', re.I),\n",
        "    re.compile(r'^\\s*Version\\s+\\d+(\\.\\d+)?\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s*$', re.I),\n",
        "    re.compile(r'^\\s*\\d+\\s*$'),\n",
        "]\n",
        "TOC_LINE_RE       = re.compile(r'.+\\.\\s?\\.\\s?\\.\\s+\\d+$')\n",
        "SECTION_BANNER_RE = re.compile(r'^[A-Z]{1,4}\\s*-\\s+.+$')\n",
        "\n",
        "\n",
        "RULE_ID_RE = re.compile(r'^\\s*[A-Z]{1,3}\\s*[-.]?\\s*\\d+(\\.\\d+)*\\b')\n",
        "UNIT_RE    = re.compile(r'\\b(\\d+(\\.\\d+)?)\\s*(mm|cm|in|inch(es)?)\\b', re.I)\n",
        "\n",
        "def extract_text_from_page(p):\n",
        "    return p.get_text(\"text\")\n",
        "\n",
        "def clean_lines(lines):\n",
        "    out = []\n",
        "    for ln in lines:\n",
        "        s = ln.rstrip().replace('\\xa0', ' ')\n",
        "        if any(rx.match(s) for rx in HEADER_RES):\n",
        "            continue\n",
        "        if TOC_LINE_RE.search(s):\n",
        "            continue\n",
        "        if SECTION_BANNER_RE.match(s):\n",
        "            continue\n",
        "        out.append(s)\n",
        "    return out\n",
        "\n",
        "def page_stats(txt: str):\n",
        "    lines = [l for l in txt.splitlines() if l.strip()]\n",
        "    n = len(lines)\n",
        "    toc_hits   = sum(1 for l in lines if TOC_LINE_RE.search(l))\n",
        "    rule_hits  = sum(1 for l in lines if RULE_ID_RE.match(l))\n",
        "    unit_hits  = sum(1 for l in lines if UNIT_RE.search(l))\n",
        "    return n, toc_hits, rule_hits, unit_hits\n",
        "\n",
        "def autodetect_start(doc, scan_first=20):\n",
        "    best = 0\n",
        "    for i in range(min(scan_first, len(doc))):\n",
        "        t = extract_text_from_page(doc[i])\n",
        "        n, toc_hits, rule_hits, unit_hits = page_stats(t)\n",
        "        if rule_hits >= 3 and toc_hits <= max(1, int(0.15 * n)):\n",
        "            return i\n",
        "        if rule_hits >= 1 and unit_hits >= 3:\n",
        "            best = i\n",
        "    return best\n",
        "\n",
        "auto_start = autodetect_start(doc)\n",
        "\n",
        "pages = []\n",
        "for i in range(auto_start, len(doc)):\n",
        "    t = extract_text_from_page(doc[i])\n",
        "    lines = clean_lines(t.splitlines())\n",
        "    if len(lines) > 0:\n",
        "        toc_like = sum(1 for l in lines if TOC_LINE_RE.search(l))\n",
        "        if toc_like > 0.3 * len(lines):\n",
        "            continue\n",
        "    pages.append(\"\\n\".join(lines))\n",
        "\n",
        "raw_text = \"\\n\".join(pages)\n",
        "print(f\"Start page (auto): {auto_start}  |  Collected text chars: {len(raw_text)}\")\n",
        "rule_chunks = {}\n",
        "current_rule = None\n",
        "buffer = []\n",
        "\n",
        "for line in raw_text.splitlines():\n",
        "    if RULE_ID_RE.match(line.strip()):\n",
        "        if current_rule and buffer:\n",
        "            rule_chunks[current_rule] = \" \".join(buffer).strip()\n",
        "        current_rule = line.strip().split()[0]   # take the rule ID\n",
        "        buffer = [line.strip()]\n",
        "    else:\n",
        "        if current_rule:\n",
        "            buffer.append(line.strip())\n",
        "\n",
        "if current_rule and buffer:\n",
        "    rule_chunks[current_rule] = \" \".join(buffer).strip()\n",
        "\n",
        "print(\"Extracted rules:\", len(rule_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import faiss\n",
        "\n",
        "rule_ids   = list(rule_chunks.keys())\n",
        "rule_texts = [rule_chunks[k] for k in rule_ids]\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "emb = model.encode(rule_texts, show_progress_bar=True, convert_to_numpy=True).astype(\"float32\")\n",
        "emb = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "index = faiss.IndexFlatIP(emb.shape[1])\n",
        "index.add(emb)\n",
        "print(\"FAISS index size:\", index.ntotal)\n",
        "\n",
        "def tokenize(s): return re.findall(r\"[a-z0-9]+\", s.lower())\n",
        "bm25 = BM25Okapi([tokenize(t) for t in rule_texts])"
      ],
      "metadata": {
        "id": "Z4cIryJTHvzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "RID_RE = re.compile(r'[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*')\n",
        "def tokenize(s: str):\n",
        "    return re.findall(r\"[a-z0-9]+\", s.lower())\n",
        "\n",
        "def extract_focus_phrase(q: str):\n",
        "    m = re.search(r'\"([^\"]+)\"', q) or re.search(r'`([^`]+)`', q)\n",
        "    return (m.group(1).strip() if m else q.strip())\n",
        "SYN = {\n",
        "    \"aerodynamics\": [\"aero\", \"wing\", \"splitter\", \"diffuser\", \"undertray\", \"endplate\", \"bodywork\"],\n",
        "    \"aerodynamic\":  [\"aero\", \"wing\", \"splitter\", \"diffuser\", \"undertray\", \"endplate\", \"bodywork\"],\n",
        "    \"impact attenuator\": [\"ia\", \"front impact\", \"energy absorber\", \"attenuator\"],\n",
        "    \"accumulator\": [\"battery pack\", \"hv\", \"shutdown\", \"ams\", \"tsal\"],\n",
        "    \"seat belt\": [\"harness\", \"restraint\", \"belts\"],\n",
        "    \"inspection\": [\"tilt\", \"brake test\", \"noise test\", \"scrutineering\"],\n",
        "}\n",
        "\n",
        "def build_cooccur_map(text, window=10, min_freq=5):\n",
        "    toks = tokenize(text)\n",
        "    pos = defaultdict(list)\n",
        "    for i, t in enumerate(toks):\n",
        "        pos[t].append(i)\n",
        "    co = defaultdict(Counter)\n",
        "    for w, idxs in pos.items():\n",
        "        for i in idxs:\n",
        "            lo, hi = max(0, i - window), min(len(toks), i + window + 1)\n",
        "            for j in range(lo, hi):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                co[w][toks[j]] += 1\n",
        "    return {w: {t: c for t, c in cnt.items() if c >= min_freq} for w, cnt in co.items()}\n",
        "\n",
        "try:\n",
        "    full_text = raw_text\n",
        "except NameError:\n",
        "    try:\n",
        "        full_text = \" \\n\".join(rule_chunks.values())\n",
        "    except NameError:\n",
        "        full_text = \"\"\n",
        "\n",
        "CO = build_cooccur_map(full_text, window=10, min_freq=5)\n",
        "def expand_focus(focus):\n",
        "    f = (focus or \"\").strip().lower()\n",
        "    extras = set(SYN.get(f, []))\n",
        "    extras.update([t for t, _c in Counter(CO.get(f, {})).most_common(6)])\n",
        "    extras = [e for e in extras if len(e) > 2]\n",
        "    return focus if not extras else f\"{focus} \" + \" \".join(sorted(set(extras))[:8])\n",
        "\n",
        "SECTION_MAP = {\n",
        "    \"aero\": [\"T.\"], \"aerodynamic\": [\"T.\"], \"aerodynamics\": [\"T.\"],\n",
        "    \"wing\": [\"T.\"], \"splitter\": [\"T.\"], \"diffuser\": [\"T.\"], \"undertray\": [\"T.\"], \"endplate\": [\"T.\"], \"bodywork\": [\"T.\"],\n",
        "    \"electrical\": [\"EV.\"], \"accumulator\": [\"EV.\"], \"hv\": [\"EV.\"], \"shutdown\": [\"EV.\"], \"insulation\": [\"EV.\"], \"ams\": [\"EV.\"], \"tsal\": [\"EV.\"],\n",
        "    \"chassis\": [\"F.\"], \"frame\": [\"F.\"], \"impact attenuator\": [\"F.\"], \"attenuator\": [\"F.\"], \"node\": [\"F.\"], \"tube\": [\"F.\"],\n",
        "    \"inspection\": [\"IN.\"], \"tilt\": [\"IN.\"], \"brake\": [\"IN.\"], \"noise\": [\"IN.\"], \"scrutineering\": [\"IN.\"],\n",
        "    \"presentation\": [\"S.\"], \"cost\": [\"C.\"], \"design\": [\"DR.\"],\n",
        "    \"engine\": [\"IC.\"], \"combustion\": [\"IC.\"], \"intake\": [\"IC.\"],\n",
        "    \"electric\": [\"EV.\"],\n",
        "}\n",
        "\n",
        "def guess_sections_from_text(text):\n",
        "    t = text.lower()\n",
        "    sections = set()\n",
        "    for k, secs in SECTION_MAP.items():\n",
        "        if k in t:\n",
        "            sections.update(secs)\n",
        "    if sections:\n",
        "        sections.add(\"GR.\")\n",
        "    return sections\n",
        "\n",
        "def retrieve_hybrid(q, k_dense=140, k_bm25=140, rrf_k=60, w_dense=1.0, w_bm25=1.35):\n",
        "    qv = model.encode([q], convert_to_numpy=True).astype(\"float32\")\n",
        "    qv = qv / (np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)\n",
        "    D, I = index.search(qv, k_dense)\n",
        "    dense_rank = {rule_ids[i]: r for r, i in enumerate(I[0], start=1)}\n",
        "    scores = bm25.get_scores(tokenize(q))\n",
        "    top = np.argsort(scores)[-k_bm25:][::-1]\n",
        "    bm25_rank = {rule_ids[i]: r for r, i in enumerate(top, start=1)}\n",
        "    def rrf(rank): return 1.0 / (rrf_k + rank)\n",
        "    fused, seen = [], set(dense_rank) | set(bm25_rank)\n",
        "    for rid in seen:\n",
        "        s = w_dense * rrf(dense_rank.get(rid, 10_000)) + w_bm25 * rrf(bm25_rank.get(rid, 10_000))\n",
        "        fused.append((s, rid))\n",
        "    fused.sort(reverse=True)\n",
        "    return [(rid, rule_chunks[rid]) for s, rid in fused]\n",
        "def soft_section_filter(cands, allowed_secs, keep_top=80, spillover=20):\n",
        "    if not allowed_secs:\n",
        "        return cands[:keep_top]\n",
        "    in_sec  = [(rid, txt) for rid, txt in cands if rid.split(\".\")[0] + \".\" in allowed_secs]\n",
        "    out_sec = [(rid, txt) for rid, txt in cands if rid.split(\".\")[0] + \".\" not in allowed_secs]\n",
        "    return in_sec[:keep_top] + out_sec[:spillover]\n",
        "\n",
        "def first_line(rid):\n",
        "    return rule_chunks[rid].splitlines()[0][:120].replace(\"\\n\", \" \")\n",
        "\n",
        "def rerank_with_gpt_strict(query, candidate_ids, max_pick=12):\n",
        "    if client is None:\n",
        "        return candidate_ids[:max_pick]\n",
        "    prompt = f\"\"\"You are an FSAE rules expert.\n",
        "From the candidate list, return ONLY rule IDs, comma-separated. At most {max_pick}.\n",
        "No extra words.\n",
        "\n",
        "Query: {query}\n",
        "Candidates: {', '.join(candidate_ids)}\n",
        "Output:\"\"\"\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        raw = (resp.choices[0].message.content or \"\").strip()\n",
        "    except Exception:\n",
        "        return candidate_ids[:max_pick]\n",
        "    ids = [x.strip().rstrip(\".\") for x in re.split(r'[,\\s]+', raw)]\n",
        "    ids = [x for x in ids if RID_RE.fullmatch(x)]\n",
        "    if not ids:\n",
        "        ids = candidate_ids[:max_pick]\n",
        "    return ids[:max_pick]\n",
        "\n",
        "def expand_hierarchy_smart(picked_ids, cands, focus=None, per_parent_cap=3, total_cap=40):\n",
        "    pool_ids = {rid for rid, _ in cands}\n",
        "    focus_kw = set(tokenize(focus or \"\"))\n",
        "    def children_of(parent):\n",
        "        pref = parent + \".\"\n",
        "        return [rid for rid in pool_ids if rid.startswith(pref)]\n",
        "    final, seen = [], set()\n",
        "    for rid in picked_ids:\n",
        "        if rid not in seen:\n",
        "            final.append(rid); seen.add(rid)\n",
        "        kids = children_of(rid)\n",
        "        scored = []\n",
        "        for kid in kids:\n",
        "            text = rule_chunks.get(kid, \"\").lower()\n",
        "            hit = sum(1 for w in focus_kw if w and w in text)\n",
        "            scored.append((hit, kid))\n",
        "        scored.sort(reverse=True)\n",
        "        for _, kid in scored[:per_parent_cap]:\n",
        "            if kid not in seen:\n",
        "                final.append(kid); seen.add(kid)\n",
        "        if len(final) >= total_cap:\n",
        "            break\n",
        "    return final[:total_cap]\n",
        "\n",
        "NEG = {\n",
        "    \"aerodynamic\": [\"presentation\", \"cost\", \"business\", \"design event\"],\n",
        "    \"aerodynamics\": [\"presentation\", \"cost\", \"business\", \"design event\"],\n",
        "}\n",
        "\n",
        "def post_filter_final(final_ids, focus_kw):\n",
        "    keep, neg = [], set()\n",
        "    for f in focus_kw:\n",
        "        neg.update(NEG.get(f, []))\n",
        "    for rid in final_ids:\n",
        "        txt = rule_chunks.get(rid, \"\").lower()\n",
        "        if any(w in txt for w in focus_kw) or len(rid.split(\".\")) <= 2:\n",
        "            if not any(n in txt for n in neg):\n",
        "                keep.append(rid)\n",
        "    return keep\n",
        "\n",
        "def normalize_ids(ids, cap=50):\n",
        "    out, seen = [], set()\n",
        "    for x in ids:\n",
        "        x = x.strip().rstrip(\".\")\n",
        "        if RID_RE.fullmatch(x) and x not in seen:\n",
        "            out.append(x); seen.add(x)\n",
        "        if len(out) >= cap:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "def natural_key(rid):\n",
        "    parts = rid.split(\".\")\n",
        "    out = [parts[0]]\n",
        "    for p in parts[1:]:\n",
        "        out.append(int(p) if p.isdigit() else p)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "Zx21tuGvKczV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = pd.read_csv(csv_file)\n",
        "compiled = []\n",
        "\n",
        "for i, row in qa_df.iterrows():\n",
        "    qid   = row.get(\"id\", f\"q_{i:04d}\")\n",
        "    q_raw = row[\"question\"]\n",
        "    gt    = row.get(\"ground_truth\",\"\")\n",
        "\n",
        "    focus_phrase = extract_focus_phrase(q_raw)\n",
        "    query_for_retrieval = expand_focus(focus_phrase)\n",
        "    allowed_secs = guess_sections_from_text(query_for_retrieval)\n",
        "\n",
        "    cands = retrieve_hybrid(query_for_retrieval, k_dense=140, k_bm25=140)\n",
        "    cands = soft_section_filter(cands, allowed_secs, keep_top=80, spillover=20)\n",
        "\n",
        "    candidate_ids = [rid for rid, _ in cands[:60]]\n",
        "    picked_ids = rerank_with_gpt_strict(query_for_retrieval, candidate_ids, max_pick=12)\n",
        "\n",
        "    final_ids = expand_hierarchy_smart(picked_ids, cands, focus=focus_phrase, per_parent_cap=3, total_cap=40)\n",
        "    focus_kw = set(tokenize(focus_phrase))\n",
        "    final_ids = post_filter_final(final_ids, focus_kw)\n",
        "    final_ids = normalize_ids(final_ids, cap=50)\n",
        "    final_ids = sorted(set(final_ids), key=natural_key)[:50]\n",
        "\n",
        "    compiled.append({\n",
        "        \"id\": qid,\n",
        "        \"question\": q_raw,\n",
        "        \"focus_used\": focus_phrase,\n",
        "        \"ground_truth\": gt,\n",
        "        \"compiled_rule_ids\": \", \".join(final_ids)\n",
        "    })\n",
        "\n",
        "    if client is not None:\n",
        "        time.sleep(0.35)\n",
        "\n",
        "out_df = pd.DataFrame(compiled)\n",
        "out_df.to_csv(\"compiled_rule_answers.csv\", index=False)\n",
        "print(\"Saved:\", \"compiled_rule_answers.csv\")"
      ],
      "metadata": {
        "id": "Tejf26haKls_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess, re, pandas as pd\n",
        "\n",
        "if not os.path.exists(\"design_qa\"):\n",
        "    !git clone -q https://github.com/anniedoris/design_qa.git\n",
        "repo_dir = os.path.abspath(\"design_qa\")\n",
        "metrics_path = os.path.join(repo_dir, \"eval\", \"metrics\", \"metrics.py\")\n",
        "\n",
        "import importlib.util\n",
        "spec = importlib.util.spec_from_file_location(\"dq_metrics\", metrics_path)\n",
        "dq_metrics = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(dq_metrics)\n",
        "\n",
        "assert 'out_df' in globals(), \"out_df is not defined. Build your predictions DataFrame first.\"\n",
        "assert {'ground_truth','compiled_rule_ids'}.issubset(out_df.columns), \\\n",
        "    \"out_df must have columns: ground_truth, compiled_rule_ids\"\n",
        "\n",
        "RID_RE = re.compile(r'[A-Z]{1,4}\\.\\d+(?:\\.\\d+)*')\n",
        "def normalize_id_list(s):\n",
        "    parts = re.split(r'[;,]\\s*', str(s))\n",
        "    ids = []\n",
        "    seen = set()\n",
        "    for p in parts:\n",
        "        x = p.strip().rstrip(\".\")\n",
        "        if RID_RE.fullmatch(x) and x not in seen:\n",
        "            ids.append(x); seen.add(x)\n",
        "    return \", \".join(ids)\n",
        "\n",
        "EVAL_CSV = \"compilation_evaluation.csv\"\n",
        "\n",
        "df_eval = pd.DataFrame({\n",
        "    \"ground_truth\": out_df[\"ground_truth\"].astype(str).str.strip(),\n",
        "    \"model_prediction\": out_df[\"compiled_rule_ids\"].fillna(\"\").map(normalize_id_list)\n",
        "})\n",
        "df_eval.to_csv(EVAL_CSV, index=False, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", EVAL_CSV)\n",
        "\n",
        "overall_f1, per_f1 = dq_metrics.eval_compilation_qa(EVAL_CSV)\n",
        "print(f\"Compilation F1 (macro): {overall_f1:.6f} on {len(per_f1)} questions\")\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(os.path.join(\"results\", \"compilation.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"DesignQA Results\\n\")\n",
        "    f.write(\"Subset: Compilation\\n\")\n",
        "    f.write(f\"F1: {overall_f1:.6f}\\n\")\n",
        "    f.write(f\"Num Questions: {len(per_f1)}\\n\")\n",
        "    try:\n",
        "        sha = subprocess.check_output([\"git\",\"-C\",\"design_qa\",\"rev-parse\",\"--short\",\"HEAD\"]).decode().strip()\n",
        "    except Exception:\n",
        "        sha = \"<git-sha>\"\n",
        "\n",
        "detail = out_df.copy()\n",
        "detail[\"model_prediction_norm\"] = df_eval[\"model_prediction\"]\n",
        "detail[\"f1\"] = per_f1\n",
        "detail.to_csv(\"compilation_detailed_with_f1.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Wrote: results/compilation.txt and compilation_detailed_with_f1.csv\")"
      ],
      "metadata": {
        "id": "k8gdjgR3KxoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
